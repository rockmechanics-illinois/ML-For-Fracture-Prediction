{"cells":[{"cell_type":"markdown","metadata":{"id":"fuHQgo374v49"},"source":["Define Hyperparameters and Paths (Then You Can Just Run All of the Cells)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5W3IqGC4x33"},"outputs":[],"source":["WELLS = [\"CCS1\", \"VW1\", \"VW2\"]\n","\n","FEATMAP_PATH = \"REU Project/Excel Sheets/Feature Map.xlsx\"\n","LAS_PATH = \"REU Project/Data/LAS Data Files\"\n","FORMATION_PATH = \"REU Project/Excel Sheets/Rock Formation Depths.xlsx\"\n","RAW_PATH = \"REU Project/Data/Raw CSV Files/raw_\"\n","NORM_PATH = \"REU Project/Data/Raw CSV Files/norm_\""]},{"cell_type":"markdown","metadata":{"id":"P67zaWVGzBma"},"source":["Imports and Installations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5833,"status":"ok","timestamp":1692065975246,"user":{"displayName":"Casey Goldberg","userId":"05984897215848063243"},"user_tz":240},"id":"FhEogY7HsFk-","outputId":"2bc5b572-7e7d-4c13-d18b-c32d9c518bb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lasio in /usr/local/lib/python3.10/dist-packages (0.31)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lasio) (1.23.5)\n"]}],"source":["pip install lasio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b21c0b7-23eb-43ba-9a3b-d0f911543a1c"},"outputs":[],"source":["import lasio\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import csv\n","import os\n","import warnings\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"XUIuoMVcr61E"},"source":["Read Feat Excel File and Get All Features that Combination of Wells Share"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1692065975447,"user":{"displayName":"Casey Goldberg","userId":"05984897215848063243"},"user_tz":240},"id":"8mJg71i4sBGl","outputId":"05714694-cc09-45e1-c59c-2f14642db288"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Features: 22\n","Features: \n","BOUND_WATER\n","CHLORITE\n","DOLOMITE\n","DTCO\n","DTSM\n","ILLITE\n","K-FELDSPAR\n","KAOLINITE\n","KSDR_PY\n","N-FELDSPAR\n","PIGE\n","PYRITE\n","QUARTZ\n","RHOZ\n","RLA2\n","RLA3\n","RLA4\n","RLA5\n","RXOZ\n","UWATER\n","XIWATER\n","XWATER\n"]}],"source":["featmap_df = pd.read_excel(FEATMAP_PATH, index_col = 0, sheet_name=\"for code\")\n","filtered_featmap = featmap_df[WELLS] #only consider pre-determined wells\n","\n","feat_mnemonics = []\n","for idx, row in filtered_featmap.iterrows():\n","  if not ((row == 0).any() | pd.isna(row)).any():\n","    #keep track of features that all wells have data for\n","    feat_mnemonics.append(idx)\n","\n","print(\"Number of Features:\", len(feat_mnemonics))\n","print(\"Features: \")\n","for feat in sorted(feat_mnemonics):\n","  print(feat)"]},{"cell_type":"markdown","metadata":{"id":"IjmnArkVBU0Q"},"source":["Read Feat Map Excel File and Get Data for Each Feature (According to Excel Sheet detailing which data file each feature should be extracted from)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X__7l3xPBR5s"},"outputs":[],"source":["well_files = {} #maps each well to its relevant files/corresponding data\n","for well in WELLS:\n","  file_to_feat = {} #maps each file to the relevant feature data it contains\n","  files = list(set(featmap_df[well].tolist())) #get well's unique files\n","  files = [file for file in files if file != 0 and not pd.isna(file)]\n","  for file in files:\n","    #write las file to a pandas df\n","    las = lasio.read(LAS_PATH + well + \"/\" + file +\".las\")\n","    las_df = pd.DataFrame(las.data, columns=las.keys())\n","\n","    #manual fix of known inconsistencies in data files\n","    las_df = las_df.rename(columns={'MD': 'DEPT'})\n","    las_df = las_df.rename(columns={'KSDRBB' : 'KSDR_PY'})\n","    las_df = las_df.rename(columns={'K_FELDSPAR' : 'K-FELDSPAR'})\n","    las_df = las_df.rename(columns={'N_FELDSPAR' : 'N-FELDSPAR'})\n","\n","    #get feature data according for files indicated by excel sheet\n","    feats_from_map = featmap_df[featmap_df[well] == file].index.tolist()\n","\n","    #filter so only considering features that all relevvant wells share\n","    shared_feats = [feat for feat in feats_from_map if feat in feat_mnemonics]\n","    feats = [\"DEPT\"] + shared_feats\n","    las_df = las_df[feats]\n","\n","    file_to_feat[file] = las_df\n","  well_files[well] = file_to_feat"]},{"cell_type":"markdown","metadata":{"id":"OawZBvIfiaK9"},"source":["Ensures All Depths are Rounded to 0.5 ft"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihbIFckr5LU6"},"outputs":[],"source":["for well, file_to_feat in well_files.items():\n","  for file, df in file_to_feat.items():\n","    df[\"DEPT\"] = (df[\"DEPT\"] * 2).round() / 2\n","    file_to_feat[file] = df\n","  well_files[well] = file_to_feat"]},{"cell_type":"markdown","metadata":{"id":"QDsvnx4HilMV"},"source":["Make One File For Each Well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBMFLgK453TL"},"outputs":[],"source":["def pd_concat(prev_df, new_df):\n","  \"\"\"\n","  Helper function that merges dataframes and takes care of duplicate features.\n","  \"\"\"\n","  if prev_df.empty:\n","    return new_df #this is first df to be concatenated\n","  else:\n","    merged_df = pd.merge(prev_df, new_df, on='DEPT', how='outer')\n","\n","    #deal with any feature duplicates (room for refinement)\n","    for feat in feat_mnemonics:\n","      if feat + \"_x\" in merged_df.columns and feat not in merged_df.columns:\n","        merged_df[feat] = \\\n","              merged_df[f'{feat}_y'].combine_first(merged_df[f'{feat}_x'])\n","        merged_df = merged_df.drop([f'{feat}_x', f'{feat}_y'], axis=1)\n","\n","    return merged_df\n","\n","\n","dfs = {} #maps well to singular, merged dataframe\n","for well, file_to_feat in well_files.items():\n","  df = pd.DataFrame()\n","  for new_df in file_to_feat.values():\n","    df = pd_concat(df, new_df)\n","\n","    #put columns in desirable order\n","    cols = sorted(df.columns.to_list())\n","    cols.remove(\"DEPT\")\n","    cols.insert(0, \"DEPT\")\n","    df = df.reindex(columns=cols)\n","\n","    dfs[well] = df\n","  dfs[well] = df"]},{"cell_type":"markdown","metadata":{"id":"Bbq_ZA_YnDkR"},"source":["Remove Rows that contain NaN values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1692066108007,"user":{"displayName":"Casey Goldberg","userId":"05984897215848063243"},"user_tz":240},"id":"nRlad4cPnFdP","outputId":"b11ce33a-dae2-4b81-9fda-4145f2936a50"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 rows were removed from well CCS1     (4217 -> 4217)\n","0 rows were removed from well VW1     (3999 -> 3999)\n","0 rows were removed from well VW2     (3413 -> 3413)\n"]}],"source":["def remove_na():\n","  for well, df in dfs.items():\n","    init_size = df.shape[0]\n","    dfs[well] = df.dropna(axis=0) #remove row if row contains a NaN\n","    fin_size = dfs[well].shape[0]\n","    print(f\"{init_size-fin_size} rows were removed from well {well} \\\n","    ({init_size} -> {fin_size})\")\n","\n","remove_na()"]},{"cell_type":"markdown","metadata":{"id":"MD3KVGzlsWqJ"},"source":["Add Rock Formation Column to Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4m8SBUmcsT1j"},"outputs":[],"source":["def add_rock_forms(dataset, well):\n","  \"\"\"\n","  Helper function that refers to pre-made excel sheet to incorporate rock\n","  formation at each depth into the dataset.\n","  \"\"\"\n","  formation_pd = pd.read_excel(FORMATION_PATH, header=0)\n","  filtered_formations = formation_pd[formation_pd[\"Well\"] == well]\n","  for index, row in dataset.iterrows():\n","    dept = row[\"DEPT\"]\n","    for index2, row2 in filtered_formations.iterrows():\n","      min_dept = row2[\"Min Dept\"]\n","      max_dept = row2[\"Max Dept\"]\n","      if dept >= min_dept and dept < max_dept:\n","        dataset.loc[index, 'FORM'] = row2[\"Structure\"]\n","\n","  return dataset\n","\n","\n","for well, df in dfs.items():\n","  dfs[well] = add_rock_forms(df, well) #add formations to dataset"]},{"cell_type":"markdown","metadata":{"id":"Ucsm_ChBnXlb"},"source":["Make Raw CSV File For Each Well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SouRVOlbnb6d"},"outputs":[],"source":["#sort data in increasing depth order for ease of interpretation\n","for well, df in dfs.items():\n","  dfs[well] = df.sort_values(by=\"DEPT\")\n","\n","#write raw data to new files\n","for well, df in dfs.items():\n","  filename = RAW_PATH + well + '.csv'\n","  df.to_csv(filename, index=False)"]},{"cell_type":"markdown","metadata":{"id":"n2_uxVj5-nsw"},"source":["Applies Min-Max Normalization to Raw Data and Writes to CSV Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4jgU8k8nd3V"},"outputs":[],"source":["def make_norm_minmax_CSV(data_sample):\n","  \"\"\"\n","  Applies min-max normalization to data column.\n","  \"\"\"\n","  max, min = np.max(data_sample), np.min(data_sample)\n","  data_sample_normalized = (data_sample - min)/(max - min)\n","  return data_sample_normalized\n","\n","\n","for well, df in dfs.items():\n","  for col in df.columns:\n","    #apply norm to each column in dataframe\n","    if col != \"DEPT\" and col != \"FORM\": #don't normalize the depths/form\n","      df[col] = make_norm_minmax_CSV(df.loc[:, col])\n","\n","#write normalized data to new files\n","for well, df in dfs.items():\n","  filename = NORM_PATH + well + '.csv'\n","  df.to_csv(filename, index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMdNrws4PE+st43X1k7Ph2R","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
